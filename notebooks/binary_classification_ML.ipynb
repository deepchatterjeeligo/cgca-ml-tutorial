{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification using scikit-learn\n",
    "\n",
    "In transient astronomy, difference imaging forms the basis of finding new transient sources in the sky. The idea simply being that the telescope survey has reference images of the patches of the sky and the nightly data is compared with the reference\n",
    "images to search for moving/non-moving transients. Moving transients involve things like asteriods while non-moving transients involve objects like supernovae (SNe) and kilo/macro-novae.\n",
    "\n",
    "The *search* is done by an image subtraction pipeline (ISP). What makes image subtraction non-trivial is the presence of noise (not surprising) in every pixel of the CCD. The ISP has to take into account the statistical fluctuations coming from the thermal noise of the instrument and also the fluctuations in photon count from an actual astrophysical source.\n",
    "\n",
    "- The bottomline, barring the the procedure of image subtractions, being that the ISP either *finds* a transient in a certain field or *does not find* it.\n",
    "- Suppose we wish to find the conditions when the ISP *finds*/*doesn't find* a transient.\n",
    "- One way (maybe the only way) to do this is to inject fake transients and then run the ISP.\n",
    "- In this case, you know the properties of a transient you are injecting. So you *know* for which cases the ISP found the transient.\n",
    "\n",
    "**But running the ISP is computationally expensive, time consuming, and most importantly, one should know how to run it!**\n",
    "\n",
    "\n",
    "So it is advantageous to have *something* that can mimic the behavior of the ISP. After all, we know roughly how the ISP should behave\n",
    "- It should find bright transients.\n",
    "- It should not find transients if the weather conditions are poor.\n",
    "- It should not find transients if the transient is in a bright galaxy.\n",
    "\n",
    "Machine learning will help us build such a decision-maker. Note that, we don't all the properties of the pipeline. We simpy want the bottomline - was the transient *found*/*not found*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries.\n",
    "- `pickle`: a format to store data/objects in a compressed form in python.\n",
    "- `numpy`: python array/vector operations library. Whenever you are writing a loop in python, double check the docs if it can be vectorized.\n",
    "- `pandas`: this is essential data origanization tool. Not native to `python`, used heavily in statistical tools like `R`. Gives easy infrastructure to organize, plot and sample (large) datasets.\n",
    "- `matplotlib`: needs no description!\n",
    "- `seaborn`: maybe needs a description :) Plotting library, customized for data analysis. Distributions, getting smoothened verions of the same, pair-wise dependence of variables in data are very easily seen. I have come to love it with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we already have (saving a couple of months) the set of fake injections which were recovered/missed from an ISP. Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/found_injections.pkl', 'rb') as f, \\\n",
    "        open('../data/all_injections.pkl', 'rb') as g:\n",
    "    df_found_complete = pickle.load(f)\n",
    "    df_all_complete = pickle.load(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not interested in the complete dataset. Let us select a subset of the columns that will be pertinent to us. Also, Let us define another dataframe which will store all the missed injections.\n",
    "\n",
    "Dataframes have useful methods for filtering. This is similar to numpy indexing with added benefits!\n",
    "The `.loc` method (not sure if I should call it a method since one calls it with `[...]` instead of `(...)`. Anyway, see [docs](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)) takes in two arguments:\n",
    "- The first one is a boolean which says which items to choose\n",
    "- The second is a list of columns to filter out\n",
    "\n",
    "I have also used one of the many useful `numpy` functions for vector operations. The `np.in1d` method (see [docs](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.in1d.html)) will take in two arrays and return an array of length of the *first* argument with boolean values depending on whether the elements of the second array are present in the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = ['stamp_mag', 'lmt_mag', 'surface_brightness', 'seeing', 'medsky']\n",
    "\n",
    "df_all = df_all_complete.loc[:, relevant_cols]\n",
    "df_found = df_found_complete.loc[:, relevant_cols]\n",
    "\n",
    "df_missed = df_all_complete.loc[\n",
    "    ~np.in1d(df_all_complete['stamp_id'].values, df_found_complete['stamp_id'].values),\n",
    "    relevant_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefix *all*, *missed* and *found* are the set of all, missed and found fake transients respectively.\n",
    "The column column descriptions are as follows:\n",
    "- `stamp_mag` is the magnitude of the transient\n",
    "- `lmt_mag` is the limiting magnitude of the instrument\n",
    "- `surface_brightness` is the surface brightness of the galaxy when the transient is simulated\n",
    "- `seeing` is the astronomical seeing at that epoch. It increases with the increasing turbulence in the atmosphere. \n",
    "- `medsky` is the median sky counts, a measure of sky brightness\n",
    "\n",
    "Generally, it is the FWHM of the point spread function of the telescope, measured in pixel units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake transients magnitudes distribution\n",
    "Let's have a look at what fake transients we injected and what was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df_all['stamp_mag'], bins=20, histtype='stepfilled', color='black', alpha=0.3, label='All')\n",
    "plt.hist(df_found['stamp_mag'], bins=20, histtype='stepfilled', color='cyan', alpha=0.8, label='Found')\n",
    "plt.hist(df_missed['stamp_mag'], bins=20, histtype='stepfilled', color='red', alpha=0.5, label='Missed')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$m_{\\mathrm{inj}}$', fontsize=15)\n",
    "plt.ylabel(r'Counts')\n",
    "plt.grid(linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word about magnitudes. This is basically a log-scale for flux of a source, defined as:\n",
    "\\begin{equation}\n",
    "    m = -2.5 \\log_{10}\\left(\\frac{F}{F_0}\\right)\n",
    "\\end{equation}\n",
    "where $F$ is the flux in particular filter while $F_0$ is some reference flux usually chosen with respect to some standard bright stars. The logarithmic scale is simply to take into account the fact that our perception follows such a measure. Think of the analogous definition of *loudness* (in units of decibel):\n",
    "\\begin{equation}\n",
    "    L = 10 \\log_{10}\\left(\\frac{I}{I_0}\\right)\n",
    "\\end{equation}\n",
    "where $I$ is the sound intensity and $I_0$ is threshold of hearing.\n",
    "\n",
    "Coming back to magnitudes, this is simply a number (no units). Brightest stars on sky have magnitude $\\simeq 1$, dimmest stars, visible to naked eye) have magnitude $\\simeq 5$. The magnitudes we are dealing with $\\simeq 20$, which is almost a factor of $10^6$ fainter in flux than the faintest we can see with our naked eye. No wonder telescopes are expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplots\n",
    "The above plot is simply the transients binned up in the transient magnitude. But, clearly, we have so many parameters which tell us about the observing conditions. The above histogram says that as the transient becomes dim, it is missed by the ISP. We could do the same distribution for galaxy surface brightness to check the dependence.\n",
    "\n",
    "How do we see them all together? Visualizing anything ($\\geq 3$)-dimensional is hard. We could, however, make *pairplots* to see the dependence. This is where `seaborn` makes our life an order of magnitude easier!\n",
    "\n",
    "Before doing that let's add a column to the `df_all` dataframe called `found` and fill it out with boolean values. After doing this, clearly, we don't need to work with the two other dataframe `df_found`, `df_missed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column and use the same in1d as used above to find the indices of found transients\n",
    "df_all = df_all.assign(found=np.in1d(df_all_complete['stamp_id'].values, df_found_complete['stamp_id'].values))\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_all,\n",
    "                 vars=['stamp_mag', 'lmt_mag', 'surface_brightness', 'seeing'],\n",
    "                 hue='found',\n",
    "                 diag_kind='auto')\n",
    "# this loop is to make the upper triangle invisible, since it is simply a redundancy.\n",
    "# feel free to remove to get the upper triangle back\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "    g.axes[i, j].set_visible(False)\n",
    "\n",
    "# The following replace the labels to something custom\n",
    "replacements = {'stamp_mag': r'$m_{\\mathrm{inj}}$', 'lmt_mag': r'$m_{\\mathrm{lim}}$',\n",
    "                'surface_brightness': r'$S_{\\mathrm{gal}} \\mathrm{mag (arcsec)^{-2}}$',\n",
    "                'seeing': r'Seeing'}\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        xlabel = g.axes[i][j].get_xlabel()\n",
    "        ylabel = g.axes[i][j].get_ylabel()\n",
    "        if xlabel in replacements.keys():\n",
    "            g.axes[i][j].set_xlabel(replacements[xlabel])\n",
    "        if ylabel in replacements.keys():\n",
    "            g.axes[i][j].set_ylabel(replacements[ylabel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots like these help us identify whether there is a *boundary* in the data, or if there is some sort of clustering into groups. In this case, we can clearly see that there is some sort of boundary for plots for $m_{\\mathrm{inj}}$ and any other parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto Machine Learning\n",
    "Let's see how well the machine is able to resolve these boundaries. \n",
    "\n",
    "There are a suite of supervised classifiers that `sklearn` provides which vary in speed and complexity. For my research, where the dataset was much larger, I have used `KNeighborsClassifier` classifier. This classifier would create a [Voronoi tessellation](https://en.wikipedia.org/wiki/Voronoi_diagram) in the parameter space about the points and make predictions on the cases you wish to test using values from the trained nearest neighbors. This type of classification models are called *non-parameteric* i.e. no model/fitting parameters involved. The decision algorithm is simple and it is computationally fast.\n",
    "\n",
    "Let's start by some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the classifier by supplying it the number of neighbors you would like to consider. I typically like to start using something like $2N + 1$, $N$ being the the number of dimensions/features in the problem. The notion simply being roughly there are 2 points for every dimension and one more to break a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we have 4 features, so let's use 9 neighbors\n",
    "num_neighbors = 9\n",
    "KNC = KNeighborsClassifier(num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_all[['stamp_mag', 'lmt_mag', 'surface_brightness', 'seeing']].values\n",
    "y_train = df_all['found'].values\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `fit` method, generic to all `sklearn` models, to initialiaze the machine with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `predict` method to make predictions now on any arbitrary parameter set. This simply gives the decision based on a majority vote across the K neighbors. Use the `predict_proba` method to get a probability, which is essentially, the fraction of K neighbors which fall in one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following is the prediction and the probability of found for a arbitrary parameter set, say,\n",
    "# stamp_mag=19, lmt_mg=20, surface_brightness=20, seeing=20\n",
    "stamp_mag=19.5\n",
    "lmt_mg=20\n",
    "sb=20\n",
    "seeing=2\n",
    "\n",
    "KNC.predict([[stamp_mag, lmt_mg, sb, seeing]]), KNC.predict_proba([[stamp_mag, lmt_mg, sb, seeing]]).T[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's essentially it!\n",
    "\n",
    "Clearly, more the number, more the computation time. Also, more the neighbors, more the bias in training. The *sweet spot* is only checked by trying out a few numbers and by cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the inaccuracy of the classifier, we could split our full dataset into training and testing samples. Say, we train on $90\\%$ of the dataset and test on $10\\%$. For the latter we can compare the true answer to the result that machine gives us.\n",
    "\n",
    "There are many submodules for testing and cross-validation in `sklearn`, let's use `train_test_split` which conveniently splits the data. Also we will repeat the entire process multiple times, say 10 times, to check for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning new variables to the data \n",
    "features = df_all[['stamp_mag', 'lmt_mag', 'surface_brightness', 'seeing']].values\n",
    "targets = df_all['found'].values\n",
    "\n",
    "# let's check an example of the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.1)\n",
    "plt.hist((X_train.T[0], X_test.T[0]), bins=30, histtype='step', color=('black', 'red'))\n",
    "plt.grid(linestyle='--')\n",
    "plt.xlabel(r'$m_{\\mathrm{inj}}$', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running train_test_split\n",
    "for ii in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, \\\n",
    "                                                        test_size=0.1, random_state=ii)\n",
    "    KNC = KNeighborsClassifier(num_neighbors)\n",
    "    KNC.fit(X_train, y_train)\n",
    "    KNC_predict = KNC.predict(X_test)\n",
    "    \n",
    "    incorrect = np.logical_xor(KNC_predict, y_test)\n",
    "    correct = np.logical_not(incorrect)\n",
    "    incorrect = np.sum(incorrect)\n",
    "    correct = np.sum(correct)\n",
    "    percentage_correct = 100.*np.float(correct)/(correct+incorrect)\n",
    "    percentage_incorrect = 100.*incorrect/(correct+incorrect)\n",
    "    \n",
    "    print(\n",
    "        \"Correct predictions {}, incorrect predictions {}, correct {}%, incorrect {}%\".format(\n",
    "        correct,\n",
    "        incorrect,\n",
    "        percentage_correct,\n",
    "        percentage_incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including correlations between parameters\n",
    "\n",
    "Sometimes the parameters that categorize the data are correlated. Let's use a different set of features and make a pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_parameters = ['stamp_mag', 'lmt_mag', 'medsky']\n",
    "g = sns.pairplot(df_all,\n",
    "                 vars=correlated_parameters,\n",
    "                 hue='found',\n",
    "                 diag_kind='auto')\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "    g.axes[i, j].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a trend in the plot between the `lmt_mag` and `medsky`. Recall, that `medsky` is the sky brightness which intuitively should lead to a poor limiting magnitude. This is seen in the plot where increasing values of limiting magnitude correlates with a dimmer sky. It would help if we could convey this information to the machine.\n",
    "\n",
    "This can be done using the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). The basic idea is to change the metric in the feature space from *Euclidean* to something which takes correlations into account a.k.a. *mahalanobis* metric. This is done automatically by the classifier given the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_all[correlated_parameters]\n",
    "targets = df_all.found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = features.cov()\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run `train_test_split` by modifying and running the classifier with mahalanobis metric. (This might take time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, \\\n",
    "                                                        test_size=0.1, random_state=ii)\n",
    "    cov = X_train.cov()\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    KNC = KNeighborsClassifier(num_neighbors,\n",
    "                               metric='mahalanobis',\n",
    "                               metric_params={'VI': inv_cov}, n_jobs=-1)\n",
    "    KNC.fit(X_train, y_train)\n",
    "    KNC_predict = KNC.predict(X_test)\n",
    "\n",
    "    incorrect = np.logical_xor(KNC_predict, y_test)\n",
    "    correct = np.logical_not(incorrect)\n",
    "    incorrect = np.sum(incorrect)\n",
    "    correct = np.sum(correct)\n",
    "    percentage_correct = 100.*np.float(correct)/(correct+incorrect)\n",
    "    percentage_incorrect = 100.*incorrect/(correct+incorrect)\n",
    "    \n",
    "    print(\n",
    "        \"Correct predictions {}, incorrect predictions {}, correct {}%, incorrect {}%\".format(\n",
    "        correct,\n",
    "        incorrect,\n",
    "        percentage_correct,\n",
    "        percentage_incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once with the standard euclidean metric (note that we have changes the features from last time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, \\\n",
    "                                                        test_size=0.1, random_state=ii)\n",
    "    KNC = KNeighborsClassifier(num_neighbors, n_jobs=-1)\n",
    "    KNC.fit(X_train, y_train)\n",
    "    KNC_predict = KNC.predict(X_test)\n",
    "\n",
    "    incorrect = np.logical_xor(KNC_predict, y_test)\n",
    "    correct = np.logical_not(incorrect)\n",
    "    incorrect = np.sum(incorrect)\n",
    "    correct = np.sum(correct)\n",
    "    percentage_correct = 100.*np.float(correct)/(correct+incorrect)\n",
    "    percentage_incorrect = 100.*incorrect/(correct+incorrect)\n",
    "    \n",
    "    print(\n",
    "        \"Correct predictions {}, incorrect predictions {}, correct {}%, incorrect {}%\".format(\n",
    "        correct,\n",
    "        incorrect,\n",
    "        percentage_correct,\n",
    "        percentage_incorrect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
